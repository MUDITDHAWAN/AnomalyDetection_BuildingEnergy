{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "DataPreProcessing_Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvVDR7QzxiWQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12f02871-68aa-4ecb-a731-1abbd187d66e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdvWfWGwxRXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGbwMZYsxRKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For reproducibility\n",
        "np.random.seed(7)\n",
        "tf.random.set_seed(7)\n",
        "random.seed(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXJjQ9EZKlAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CAN BE CHANGED \n",
        "\n",
        "dir_dataset = \"/content/drive/My Drive/Anomaly detection - BuildSys2020/\"\n",
        "dir_labels =  \"/content/drive/My Drive/ASHRAEData/\"\n",
        "dir_save_data = \"/content/drive/My Drive/ASHRAEData/data/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkgdptDh4XsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CAN BE CHANGED \n",
        "\n",
        "## change these lists as per model's input \n",
        "site_id = 0 \n",
        "\n",
        "## features to be used in the model ('anomaly' included in the list for the labels)\n",
        "features_used = ['building_id','timestamp', 'month', 'anomaly','meter_reading_log']\n",
        "\n",
        "## columns to be normalized using Z algorithm\n",
        "z_columns = []          \n",
        "\n",
        "## columns for which min-max normalization\n",
        "minmax_columns = ['month']\n",
        "\n",
        "## window length \n",
        "seq_length = 24 \n",
        "\n",
        "## number of buildings to be considered if not using site id\n",
        "nb_buildings = 145\n",
        "\n",
        "## output feature to be predicted \n",
        "output_feature = 'meter_reading_log'\n",
        "\n",
        "## name of the model for which data is prepared\n",
        "model_name = 'Baseline'\n",
        "\n",
        "## for splitting the non-Anomalous data \n",
        "train_percent = 0.8\n",
        "val_percent = 0.1\n",
        "test_percent = 0.1\n",
        "\n",
        "## for splitting the Anomalous data \n",
        "train_percent_anom = 0.1\n",
        "val_percent_anom = 0.2\n",
        "test_percent_anom = 0.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0NU2rE-LVg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Reading dataset \n",
        "os.chdir(dir_dataset)\n",
        "with open('ashrae_rank1_train_clean.pkl', 'rb') as f:\n",
        "  dataset = pickle.load(f)\n",
        "\n",
        "## Reading Curated Anomaly Labels \n",
        "os.chdir(dir_labels)\n",
        "anomaly_labels = pd.read_csv(\"anomaly_labels.csv\")\n",
        "\n",
        "## Concatenating the two \n",
        "dataset = pd.concat([dataset, anomaly_labels], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rb-lDwvxRDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def usable_features(df_building, features_used):  # for droping the features not to be considered in the model\n",
        "  ## df_building - pandas dataframe of the datset\n",
        "  ## features_used - features to be included in the cleaned dataset\n",
        "\n",
        "  # creating list of columns that are not used \n",
        "  col = df_building.columns\n",
        "  features_not_used = [] # store columns that won't be used\n",
        "  for x in col:\n",
        "    if x not in features_used:\n",
        "      features_not_used.append(x)\n",
        "\n",
        "  # drop the columns\n",
        "  df_building= df_building.drop(features_not_used, axis = 1) # dropping not used features \n",
        "\n",
        "  # to fill NA data we simply replace the values with 0\n",
        "  df_building = df_building.fillna(0)\n",
        "  \n",
        "  return df_building"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qGiZRhVxb53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_minmax(df, col):  # function to normalize the columns \n",
        "    ## df - data frame containing the columns\n",
        "    ## columns to be normalized  \n",
        "    result = df.copy()\n",
        "    for feature_name in col:\n",
        "        max_value = df[feature_name].max()\n",
        "        min_value = df[feature_name].min()\n",
        "\n",
        "        print(\"Feature Name : {}  :  Max Value - {} ; Min Value - {}\".format(feature_name, max_value, min_value))\n",
        "        # normalize \n",
        "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
        "    return result "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtN1AvLI2kWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_zAlgo(df, col):  # function to normalize the columns \n",
        "    ## df - data frame containing the columns\n",
        "    ## columns to be normalized  \n",
        "    result = df.copy()\n",
        "    for feature_name in col:\n",
        "        std_value = df[feature_name].std()\n",
        "        mean_value = df[feature_name].mean()\n",
        "\n",
        "        print(\"Feature Name : {}  :  Std Value - {} ; Mean Value - {}\".format(feature_name, std_value, mean_value))\n",
        "        # normalize \n",
        "        result[feature_name] = (df[feature_name] - mean_value) / std_value\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3syDC9Amxbuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_dataset_ashrae(dataset, site_id, features_used, z_columns, minmax_columns):\n",
        "  ## site_id - site id be included \n",
        "  ## features_used - list of features to be included in the final data\n",
        "  ## z_columns - list of columns to be normalized with Z algorithm\n",
        "  ## minmax_columns - list of columns to be normalized with min-max normalization\n",
        "\n",
        "  ## divide by site ids\n",
        "  dataset = dataset.loc[dataset['site_id'] == site_id]\n",
        "  \n",
        "  meter_dataset = dataset.loc[dataset['meter']==0]\n",
        "  \n",
        "  # create a new column  \n",
        "  meter_dataset['meter_reading_log'] = np.log(meter_dataset['meter_reading']+1) \n",
        "\n",
        "  #convert categorical data type to int\n",
        "  meter_dataset['primary_use'] = meter_dataset['primary_use'].astype('category')\n",
        "  unique_primUse_list = list(meter_dataset.primary_use.unique())\n",
        "  unique_primUse_dict = {unique_primUse_list[i]: i for i in range(0, len(unique_primUse_list))}\n",
        "  meter_dataset['primary_use'] = meter_dataset['primary_use'].map(unique_primUse_dict).astype(int)\n",
        "\n",
        "  # remove columns that won't be included in the model's input \n",
        "  df_train = usable_features(meter_dataset, features_used)\n",
        "\n",
        "  # normalize features \n",
        "  df_train_norm1 = normalize_zAlgo(df_train, z_columns) # z-algorithm\n",
        "  df_train_norm = normalize_zAlgo(df_train_norm1, minmax_columns) # min-max scaler\n",
        "\n",
        "  print(\"df_train_norm shape {}\".format(df_train_norm.shape))\n",
        "  return df_train_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKGwrhmWiLKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## creating input sequence of length - seq_length\n",
        "def create_windows(arr_data, arr_anomaly_label, arr_op, seq_length, nb_features):\n",
        "  ## arr_data - normalized dataset as a numpy array \n",
        "  ## arr_op - array containing anomaly labels for each time step \n",
        "  ## seq_length - wiindow length \n",
        "  ## nb_features - no. of features to be used as input \n",
        "\n",
        "  anom_x = [] # storing anomalous windows - input \n",
        "  non_anom_x = [] # storing non-anomalous windows - input \n",
        "  anom_y = [] # storing anomalous windows - output \n",
        "  non_anom_y = [] # storing non-anomalous windows - output \n",
        "\n",
        "  # run a loop to move a seq_length size window across data non-overlapping\n",
        "  for i in range(0,arr_data.shape[0]//seq_length):\n",
        "\n",
        "    # slice the window \n",
        "    window_features = arr_data[i*seq_length:(i+1)*seq_length].reshape((seq_length, nb_features))   # window of seq_length\n",
        "    window_output = arr_op[i*seq_length:(i+1)*seq_length].reshape((seq_length,1))\n",
        "\n",
        "    cond_input = np.concatenate((window_output, window_features[0][0].reshape((1,1))))\n",
        "\n",
        "    is_anomaly = np.count_nonzero(arr_anomaly_label[i*seq_length:(i+1)*seq_length])  #if even at one time point anomaly is present the window would be considered anomalous\n",
        "    # print(is_anomaly)\n",
        "\n",
        "    if is_anomaly > 0 :  # separating the anomalous and non-anomalous data \n",
        "      anom_x.append(cond_input)\n",
        "      anom_y.append(window_output)\n",
        "    else:\n",
        "      non_anom_x.append(cond_input)\n",
        "      non_anom_y.append(window_output)\n",
        "\n",
        "  return non_anom_x, non_anom_y, anom_x, anom_y "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7HUnaYIxbfr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mix_data(data_x, data_y):\n",
        "  #  Mix Data (to make it similar to i.i.d)\n",
        "  data_idx = np.random.permutation(len(data_x))\n",
        "\n",
        "\n",
        "  output_data_x = []  # Store shuffled data\n",
        "  output_data_y = []\n",
        "\n",
        "  for i in range(len(data_x)):\n",
        "    output_data_x.append(data_x[data_idx[i]])\n",
        "    output_data_y.append(data_y[data_idx[i]])\n",
        "\n",
        "  print(\"ouput_x shape {} , {}\".format(len(output_data_x), output_data_x[0].shape))\n",
        "  return output_data_x, output_data_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APqiUfxZxbTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data_ashrae(dataset, site_id, features_used, z_columns, minmax_columns, seq_length, nb_buildings, output_feature):\n",
        "  ## dataset - dataset DataFrame\n",
        "  ## site_id - site_id to be considered\n",
        "  ## seq_length - window length \n",
        "  ## nb_buildings - no of buildings for the input samples \n",
        "  ## output_feature - feature that would be taken as the output to be predicted \n",
        "\n",
        "  df_final_train_norm = clean_dataset_ashrae(dataset, site_id, features_used, z_columns, minmax_columns)\n",
        "\n",
        "  # lists to store final input data  \n",
        "  anom_x = []\n",
        "  anom_y = []\n",
        "  non_anom_x = []\n",
        "  non_anom_y = []\n",
        "\n",
        "  # group data on the basis of their building_id \n",
        "  grp_data = df_final_train_norm.groupby('building_id')\n",
        "\n",
        "  # create a random list of building whose data would be considered \n",
        "  # list_buildings = random.sample(list(grp_data.groups.keys()), nb_buildings)\n",
        "\n",
        "  ## all buildings in the site \n",
        "  list_buildings = list(grp_data.groups.keys())\n",
        "\n",
        "  # loop throught the building ids - segregate them into Anomalous and Non-Anomalouws windows\n",
        "  for grp_no in list_buildings:\n",
        "\n",
        "    # pick a building id whose data needs to be added into final input data\n",
        "    building_data = grp_data.get_group(grp_no)\n",
        "    #building_id is not needed anymore \n",
        "    building_data = building_data.drop('building_id', axis = 1) # dropping not used features \n",
        "\n",
        "    # separating labels from the data \n",
        "    arr_labels = building_data['anomaly'].to_numpy()\n",
        "    # label is not not need anymore \n",
        "    building_data = building_data.drop('anomaly', axis = 1) # dropping not used features \n",
        "\n",
        "    # separating output_feature from the data \n",
        "    arr_output = building_data[output_feature].to_numpy()\n",
        "    # output_feature is not not need anymore \n",
        "    building_data = building_data.drop(output_feature, axis = 1) # dropping not used features \n",
        "\n",
        "    # making sure the data is sequential \n",
        "    building_data.sort_values(\"timestamp\", axis = 0, ascending = True) \n",
        "    building_data = building_data.drop('timestamp', axis = 1) # dropping not used features \n",
        "\n",
        "    # creating numpy array of remaining features \n",
        "    building_data = building_data.to_numpy()\n",
        "\n",
        "    # creating windowed data\n",
        "    nb_features = building_data.shape[1]\n",
        "    na_x, na_y, a_x, a_y = create_windows(building_data, arr_labels, arr_output, seq_length, nb_features)\n",
        "\n",
        "    # print(\" na_x - len {} \".format(len(na_x)))\n",
        "    # print(\" a_x - len {}\".format(len(a_x)))\n",
        "    # accumulating single bulding data \n",
        "    anom_x.extend(a_x)\n",
        "    anom_y.extend(a_y)\n",
        "    non_anom_x.extend(na_x)\n",
        "    non_anom_y.extend(na_y)\n",
        "\n",
        "  # making data similar to i.i.d.\n",
        "  anom_x, anom_y = mix_data(anom_x, anom_y)\n",
        "  non_anom_x, non_anom_y = mix_data(non_anom_x, non_anom_y)\n",
        "\n",
        "  return non_anom_x, non_anom_y, anom_x, anom_y "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jf42zRP_DPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data(train_percent, val_percent, test_percent, output_non_anom_x, output_non_anom_y ):\n",
        "  ## function for splitting the data into train, val and test\n",
        "  \n",
        "  # train dataset\n",
        "  X_train_non_anom = output_non_anom_x[:int((len(output_non_anom_y)*train_percent))]\n",
        "  Y_train_non_anom = output_non_anom_y[:int((len(output_non_anom_y)*train_percent))]\n",
        "\n",
        "  # validation dataset\n",
        "  X_val_non_anom = output_non_anom_x[int((len(output_non_anom_y)*train_percent)):-int((len(output_non_anom_y)*test_percent) )]\n",
        "  Y_val_non_anom = output_non_anom_y[int((len(output_non_anom_y)*train_percent)):-int((len(output_non_anom_y)*test_percent) )]\n",
        "\n",
        "  # test dataset                                                          \n",
        "  X_test_non_anom = output_non_anom_x[-int(len(output_non_anom_y)*test_percent):]\n",
        "  Y_test_non_anom = output_non_anom_y[-int(len(output_non_anom_y)*test_percent):]     \n",
        "\n",
        "  return  X_train_non_anom, Y_train_non_anom, X_val_non_anom, Y_val_non_anom, X_test_non_anom, Y_test_non_anom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtFXHogzxbK1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "9b620b8a-d736-43fa-80c6-b3e86f15eb3a"
      },
      "source": [
        "## loading the data ( calling the function here )\n",
        "non_anom_x, non_anom_y, anom_x, anom_y = load_data_ashrae(dataset, site_id, features_used, z_columns, minmax_columns, seq_length, nb_buildings, output_feature)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Feature Name : month  :  Std Value - 3.4521200118787347 ; Mean Value - 6.529371681698442\n",
            "df_train_norm shape (908409, 5)\n",
            "ouput_x shape 22919 , (25, 1)\n",
            "ouput_x shape 14927 , (25, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GruIOx1A8fZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Splitting Non-Anomalous data \n",
        "X_train_non_anom, Y_train_non_anom, X_val_non_anom, Y_val_non_anom, X_test_non_anom, Y_test_non_anom = split_data(train_percent, val_percent, test_percent, non_anom_x, non_anom_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdYX6vM-MD4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Saving Non- Anomalous data - Conditional\n",
        "os.chdir(dir_save_data)\n",
        "if not os.path.exists(model_name + \"_data/\"):\n",
        "  os.mkdir(model_name + \"_data/\")\n",
        "\n",
        "if not os.path.exists(model_name + \"_data/Conditional/\"):\n",
        "  os.mkdir(model_name + \"_data/Conditional/\")\n",
        "\n",
        "if not os.path.exists(model_name + \"_data/Conditional/\" + \"site_id_\" + str(site_id)+\"/\"):\n",
        "  os.mkdir(model_name + \"_data/Conditional/\" + \"site_id_\" + str(site_id)+\"/\")\n",
        "\n",
        "if not os.path.exists(model_name + \"_data/Conditional/\" + \"site_id_\" + str(site_id)+\"/non_anom/\"):\n",
        "  os.mkdir(model_name + \"_data/Conditional/\" + \"site_id_\" + str(site_id)+\"/non_anom/\")\n",
        "\n",
        "## Saving training data\n",
        "with open(\"./\" + model_name + \"_data/Conditional/\"+ \"site_id_\" + str(site_id)+\"/non_anom/X_train_data.pkl\", 'wb') as f:\n",
        "  pickle.dump(X_train_non_anom, f)\n",
        "\n",
        "with open(\"./\" + model_name + \"_data/Conditional/\"+ \"site_id_\" + str(site_id)+\"/non_anom/Y_train_data.pkl\", 'wb') as f:\n",
        "  pickle.dump(Y_train_non_anom, f)\n",
        "\n",
        "## Saving validation data\n",
        "with open(\"./\" + model_name + \"_data/Conditional/\"+ \"site_id_\" + str(site_id)+\"/non_anom/X_val_data.pkl\", 'wb') as f:\n",
        "  pickle.dump(X_val_non_anom, f)\n",
        "\n",
        "with open(\"./\" + model_name + \"_data/Conditional/\"+ \"site_id_\" + str(site_id)+\"/non_anom/Y_val_data.pkl\", 'wb') as f:\n",
        "  pickle.dump(Y_val_non_anom, f)\n",
        "\n",
        "## Saving test data\n",
        "with open(\"./\" + model_name + \"_data/Conditional/\"+ \"site_id_\" + str(site_id)+\"/non_anom/X_test_data.pkl\", 'wb') as f:\n",
        "  pickle.dump(X_test_non_anom, f)\n",
        "\n",
        "with open(\"./\" + model_name + \"_data/Conditional/\"+ \"site_id_\" + str(site_id)+\"/non_anom/Y_test_data.pkl\", 'wb') as f:\n",
        "  pickle.dump(Y_test_non_anom, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgW7Jt2BMDuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Saving Non- Anomalous data - Not Conditional\n",
        "os.chdir(dir_save_data)\n",
        "if not os.path.exists(model_name + \"_data/Not_Conditional/\"):\n",
        "  os.mkdir(model_name + \"_data/Not_Conditional/\")\n",
        "\n",
        "if not os.path.exists(model_name + \"_data/Not_Conditional/\" + \"site_id_\" + str(site_id)+\"/\"):\n",
        "  os.mkdir(model_name + \"_data/Not_Conditional/\" + \"site_id_\" + str(site_id)+\"/\")\n",
        "\n",
        "if not os.path.exists(model_name + \"_data/Not_Conditional/\" + \"site_id_\" + str(site_id)+\"/non_anom/\"):\n",
        "  os.mkdir(model_name + \"_data/Not_Conditional/\" + \"site_id_\" + str(site_id)+\"/non_anom/\")\n",
        "\n",
        "## Saving training data\n",
        "with open(\"./\" + model_name + \"_data/Not_Conditional/\"+ \"site_id_\" + str(site_id)+\"/non_anom/train_data.pkl\", 'wb') as f:\n",
        "  pickle.dump(Y_train_non_anom, f)\n",
        "\n",
        "## Saving validation data\n",
        "with open(\"./\" + model_name + \"_data/Not_Conditional/\"+ \"site_id_\" + str(site_id)+\"/non_anom/val_data.pkl\", 'wb') as f:\n",
        "  pickle.dump(Y_val_non_anom, f)\n",
        "\n",
        "## Saving test data\n",
        "with open(\"./\" + model_name + \"_data/Not_Conditional/\"+ \"site_id_\" + str(site_id)+\"/non_anom/test_data.pkl\", 'wb') as f:\n",
        "  pickle.dump(Y_test_non_anom, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyEGpQ--mipl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Splitting the Anomalous data\n",
        "X_train_anom, Y_train_anom, X_val_anom, Y_val_anom, X_test_anom, Y_test_anom = split_data(train_percent_anom, val_percent_anom, test_percent_anom, anom_x, anom_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN6COB5MNKWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Saving Anomalous data - Conditional Data\n",
        "os.chdir(dir_save_data)\n",
        "if not os.path.exists(model_name + \"_data/Conditional/\" + \"site_id_\" + str(site_id)+\"/anom/\"):\n",
        "  os.mkdir(model_name + \"_data/Conditional/\" + \"site_id_\" + str(site_id)+\"/anom/\")\n",
        "\n",
        "## Saving training data\n",
        "with open(\"./\" + model_name + \"_data/Conditional/\"+ \"site_id_\" + str(site_id)+\"/anom/X_train_data.pkl\", 'wb') as f:\n",
        "  pickle.dump(X_train_anom, f)\n",
        "\n",
        "with open(\"./\" + model_name + \"_data/Conditional/\"+ \"site_id_\" + str(site_id)+\"/anom/Y_train_data.pkl\", 'wb') as f:\n",
        "  pickle.dump(Y_train_anom, f)\n",
        "\n",
        "## Saving validation data\n",
        "with open(\"./\" + model_name + \"_data/Conditional/\"+ \"site_id_\" + str(site_id)+\"/anom/X_val_data.pkl\", 'wb') as f:\n",
        "  pickle.dump(X_val_anom, f)\n",
        "\n",
        "with open(\"./\" + model_name + \"_data/Conditional/\"+ \"site_id_\" + str(site_id)+\"/anom/Y_val_data.pkl\", 'wb') as f:\n",
        "  pickle.dump(Y_val_anom, f)\n",
        "\n",
        "## Saving test data\n",
        "with open(\"./\" + model_name + \"_data/Conditional/\"+ \"site_id_\" + str(site_id)+\"/anom/X_test_data.pkl\", 'wb') as f:\n",
        "  pickle.dump(X_test_anom, f)\n",
        "\n",
        "with open(\"./\" + model_name + \"_data/Conditional/\"+ \"site_id_\" + str(site_id)+\"/anom/Y_test_data.pkl\", 'wb') as f:\n",
        "  pickle.dump(Y_test_anom, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG_pVihBNKIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Saving Anomalous data - Not Conditional\n",
        "os.chdir(dir_save_data)\n",
        "if not os.path.exists(model_name + \"_data/Not_Conditional/\" + \"site_id_\" + str(site_id)+\"/anom/\"):\n",
        "  os.mkdir(model_name + \"_data/Not_Conditional/\" + \"site_id_\" + str(site_id)+\"/anom/\")\n",
        "\n",
        "## Saving training data\n",
        "with open(\"./\" + model_name + \"_data/Not_Conditional/\"+ \"site_id_\" + str(site_id)+\"/anom/train_data.pkl\", 'wb') as f:\n",
        "  pickle.dump(Y_train_anom, f)\n",
        "\n",
        "## Saving validation data\n",
        "with open(\"./\" + model_name + \"_data/Not_Conditional/\"+ \"site_id_\" + str(site_id)+\"/anom/val_data.pkl\", 'wb') as f:\n",
        "  pickle.dump(Y_val_anom, f)\n",
        "\n",
        "## Saving test data\n",
        "with open(\"./\" + model_name + \"_data/Not_Conditional/\"+ \"site_id_\" + str(site_id)+\"/anom/test_data.pkl\", 'wb') as f:\n",
        "  pickle.dump(Y_test_anom, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBmYWJdUOoF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}