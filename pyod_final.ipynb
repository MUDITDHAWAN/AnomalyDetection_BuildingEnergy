{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "pyod_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvVDR7QzxiWQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "08598a3f-795f-483a-eddf-af5f5cf9de4a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdvWfWGwxRXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHtb-3pxxRQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"/content/drive/My Drive/Anomaly detection - BuildSys2020/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGbwMZYsxRKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For reproducibility\n",
        "np.random.seed(12)\n",
        "tf.random.set_seed(12)\n",
        "random.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rb-lDwvxRDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def usable_features(df_building, features_used):  # for droping the features not to be considered in the model\n",
        "  ## df_building - pandas dataframe of the datset\n",
        "  ## features_used - features to be included in the cleaned dataset\n",
        "\n",
        "  # creating list of columns that are not used \n",
        "  col = df_building.columns\n",
        "  features_not_used = [] # store columns that won't be used\n",
        "  for x in col:\n",
        "    if x not in features_used:\n",
        "      features_not_used.append(x)\n",
        "\n",
        "  # drop the columns\n",
        "  df_building= df_building.drop(features_not_used, axis = 1) # dropping not used features \n",
        "\n",
        "  # to fill NA data we simply replace the values with 0\n",
        "  df_building = df_building.fillna(0)\n",
        "  \n",
        "  return df_building"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qGiZRhVxb53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_minmax(df, col):  # function to normalize the columns \n",
        "    ## df - data frame containing the columns\n",
        "    ## columns to be normalized  \n",
        "    result = df.copy()\n",
        "    for feature_name in col:\n",
        "        max_value = df[feature_name].max()\n",
        "        min_value = df[feature_name].min()\n",
        "\n",
        "        print(\"Feature Name : {}  :  Max Value - {} ; Min Value - {}\".format(feature_name, max_value, min_value))\n",
        "        # normalize \n",
        "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
        "    return result "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtN1AvLI2kWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_zAlgo(df, col):  # function to normalize the columns \n",
        "    ## df - data frame containing the columns\n",
        "    ## columns to be normalized  \n",
        "    result = df.copy()\n",
        "    for feature_name in col:\n",
        "        std_value = df[feature_name].std()\n",
        "        mean_value = df[feature_name].mean()\n",
        "\n",
        "        print(\"Feature Name : {}  :  Std Value - {} ; Mean Value - {}\".format(feature_name, std_value, mean_value))\n",
        "        # normalize \n",
        "        result[feature_name] = (df[feature_name] - mean_value) / std_value\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3syDC9Amxbuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_dataset_ashrae(dataset, meter_types, features_used, z_columns, minmax_columns):\n",
        "  ## meter_types - list of meters to be included \n",
        "  ## features_used - list of features to be included in the final data\n",
        "  ## z_columns - list of columns to be normalized with Z algorithm\n",
        "  ## minmax_columns - list of columns to be normalized with min-max normalization\n",
        "\n",
        "  meter_dataset = dataset.loc[dataset['meter']==0]\n",
        "  \n",
        "  # create a new column  \n",
        "  meter_dataset['meter_reading_log'] = np.log(meter_dataset['meter_reading']+1) \n",
        "\n",
        "  #convert categorical data type to int\n",
        "  meter_dataset['primary_use'] = meter_dataset['primary_use'].astype('category')\n",
        "  unique_primUse_list = list(meter_dataset.primary_use.unique())\n",
        "  unique_primUse_dict = {unique_primUse_list[i]: i for i in range(0, len(unique_primUse_list))}\n",
        "  meter_dataset['primary_use'] = meter_dataset['primary_use'].map(unique_primUse_dict).astype(int)\n",
        "\n",
        "  # remove columns that won't be included in the model's input \n",
        "  df_train = usable_features(meter_dataset, features_used)\n",
        "\n",
        "  # normalize features \n",
        "  df_train_norm1 = normalize_zAlgo(df_train, z_columns) # z-algorithm\n",
        "  df_train_norm = normalize_zAlgo(df_train_norm1, minmax_columns) # min-max scaler\n",
        "\n",
        "  print(\"df_train_norm shape {}\".format(df_train_norm.shape))\n",
        "  return df_train_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKGwrhmWiLKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## creating input sequence of length - seq_length\n",
        "def create_windows(arr_data, arr_anomaly_label, arr_op, seq_length, nb_features):\n",
        "  ## arr_data - normalized dataset as a numpy array \n",
        "  ## arr_op - array containing anomaly labels for each time step \n",
        "  ## seq_length - wiindow length \n",
        "  ## nb_features - no. of features to be used as input \n",
        "\n",
        "  anom_x = [] # storing anomalous windows - input \n",
        "  non_anom_x = [] # storing non-anomalous windows - input \n",
        "  anom_y = [] # storing anomalous windows - output \n",
        "  non_anom_y = [] # storing non-anomalous windows - output \n",
        "\n",
        "  # run a loop to move a seq_length size window across data non-overlapping\n",
        "  for i in range(0,arr_data.shape[0]//seq_length):\n",
        "\n",
        "    # slice the window \n",
        "    window_features = arr_data[i*seq_length:(i+1)*seq_length].reshape((seq_length, nb_features))   # window of seq_length\n",
        "    window_output = arr_op[i*seq_length:(i+1)*seq_length].reshape((seq_length,1))\n",
        "\n",
        "    is_anomaly = np.count_nonzero(arr_anomaly_label[i*seq_length:(i+1)*seq_length])  #if even at one time point anomaly is present the window would be considered anomalous\n",
        "    # print(is_anomaly)\n",
        "\n",
        "    if is_anomaly > 0 :  # separating the anomalous and non-anomalous data \n",
        "      anom_x.append(window_features)\n",
        "      anom_y.append(window_output)\n",
        "    else:\n",
        "      non_anom_x.append(window_features)\n",
        "      non_anom_y.append(window_output)\n",
        "\n",
        "  return non_anom_x, non_anom_y, anom_x, anom_y "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7HUnaYIxbfr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mix_data(data_x, data_y):\n",
        "  #  Mix Data (to make it similar to i.i.d)\n",
        "  data_idx = np.random.permutation(len(data_x))\n",
        "\n",
        "\n",
        "  output_data_x = []  # Store shuffled data\n",
        "  output_data_y = []\n",
        "\n",
        "  for i in range(len(data_x)):\n",
        "    output_data_x.append(data_x[data_idx[i]])\n",
        "    output_data_y.append(data_y[data_idx[i]])\n",
        "\n",
        "  print(\"ouput_x shape {} , {}\".format(len(output_data_x), output_data_x[0].shape))\n",
        "  return output_data_x, output_data_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f0BWA_XINKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('ashrae_rank1_train_clean.pkl', 'rb') as f:\n",
        "      dataset = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APqiUfxZxbTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data_ashrae(dataset, meter_types, features_used, z_columns, minmax_columns, seq_length, nb_buildings, output_feature):\n",
        "  ## seq_length - window length \n",
        "  ## nb_buildings - no of buildings for the input samples \n",
        "  ## output_feature - feature that would be taken as the output to be predicted \n",
        "\n",
        "  ## import the dataset \n",
        "  \n",
        "\n",
        "  df_final_train_norm = clean_dataset_ashrae(dataset, meter_types, features_used, z_columns, minmax_columns)\n",
        "\n",
        "  # lists to store final input data  \n",
        "  anom_x = []\n",
        "  anom_y = []\n",
        "  non_anom_x = []\n",
        "  non_anom_y = []\n",
        "\n",
        "  # group data on the basis of their building_id \n",
        "  grp_data = df_final_train_norm.groupby('building_id')\n",
        "\n",
        "  # create a random list of building whose data would be considered \n",
        "  list_buildings = random.sample(list(grp_data.groups.keys()), nb_buildings)\n",
        "\n",
        "  # loop throught the building ids - segregate them into Anomalous and Non-Anomalouws windows\n",
        "  for grp_no in list_buildings:\n",
        "\n",
        "    # pick a building id whose data needs to be added into final input data\n",
        "    building_data = grp_data.get_group(grp_no)\n",
        "    #building_id is not needed anymore \n",
        "    building_data = building_data.drop('building_id', axis = 1) # dropping not used features \n",
        "\n",
        "    # separating labels from the data \n",
        "    arr_labels = building_data['is_bad_meter_reading'].to_numpy()\n",
        "    # label is not not need anymore \n",
        "    building_data = building_data.drop('is_bad_meter_reading', axis = 1) # dropping not used features \n",
        "\n",
        "    # separating output_feature from the data \n",
        "    arr_output = building_data[output_feature].to_numpy()\n",
        "    # output_feature is not not need anymore \n",
        "    building_data = building_data.drop(output_feature, axis = 1) # dropping not used features \n",
        "\n",
        "    # making sure the data is sequential \n",
        "    building_data.sort_values(\"timestamp\", axis = 0, ascending = True) \n",
        "    building_data = building_data.drop('timestamp', axis = 1) # dropping not used features \n",
        "\n",
        "    # creating numpy array of remaining features \n",
        "    building_data = building_data.to_numpy()\n",
        "\n",
        "    # creating windowed data\n",
        "    nb_features = building_data.shape[1]\n",
        "    na_x, na_y, a_x, a_y = create_windows(building_data, arr_labels, arr_output, seq_length, nb_features)\n",
        "\n",
        "    print(\" na_x - len {}, shape {} \".format(len(na_x), na_x[0].shape))\n",
        "    print(\" a_x - len {}\".format(len(a_x)))\n",
        "    # accumulating single bulding data \n",
        "    anom_x.extend(a_x)\n",
        "    anom_y.extend(a_y)\n",
        "    non_anom_x.extend(na_x)\n",
        "    non_anom_y.extend(na_y)\n",
        "\n",
        "  # making data similar to i.i.d.\n",
        "  anom_x, anom_y = mix_data(anom_x, anom_y)\n",
        "  non_anom_x, non_anom_y = mix_data(non_anom_x, non_anom_y)\n",
        "\n",
        "  return non_anom_x, non_anom_y, anom_x, anom_y "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkgdptDh4XsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## change these lists as per model's input \n",
        "meter_types = [0] \n",
        "\n",
        "features_used = ['building_id','timestamp', 'air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', \n",
        "                  'sea_level_pressure', 'wind_speed','primary_use', 'square_feet', 'year_built', 'floor_count', \n",
        "                  'hour', 'weekday', 'month', 'is_bad_meter_reading','meter_reading_log']\n",
        "\n",
        "z_columns = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure', 'wind_speed', \n",
        "              'square_feet', 'year_built' ]          \n",
        "\n",
        "minmax_columns = ['hour', 'month', 'weekday']\n",
        "\n",
        "seq_length = 24 \n",
        "\n",
        "\n",
        "nb_buildings = 145\n",
        "\n",
        "output_feature = 'meter_reading_log'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtFXHogzxbK1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "719cd0c7-88cc-4e65-b55b-7d3c04004465"
      },
      "source": [
        "non_anom_x, non_anom_y, anom_x, anom_y = load_data_ashrae(dataset, meter_types, features_used, z_columns, minmax_columns, seq_length, nb_buildings, output_feature)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Feature Name : air_temperature  :  Std Value - 10.692255020141602 ; Mean Value - 15.730816841125488\n",
            "Feature Name : cloud_coverage  :  Std Value - 125.52488356038128 ; Mean Value - 114.02984235849534\n",
            "Feature Name : dew_temperature  :  Std Value - 10.18441104888916 ; Mean Value - 8.148475646972656\n",
            "Feature Name : precip_depth_1_hr  :  Std Value - 6.967662172348021 ; Mean Value - 0.21803520629869555\n",
            "Feature Name : sea_level_pressure  :  Std Value - 18.378942489624023 ; Mean Value - 1023.1770629882812\n",
            "Feature Name : wind_speed  :  Std Value - 2.313711404800415 ; Mean Value - 3.6503612995147705\n",
            "Feature Name : square_feet  :  Std Value - 112109.99461909568 ; Mean Value - 92714.31195025914\n",
            "Feature Name : year_built  :  Std Value - 95.44950642767324 ; Mean Value - 168.47074200868758\n",
            "Feature Name : hour  :  Std Value - 6.922763943175829 ; Mean Value - 11.500643317958595\n",
            "Feature Name : month  :  Std Value - 3.4435748699115054 ; Mean Value - 6.552057846381409\n",
            "Feature Name : weekday  :  Std Value - 1.9972787897842907 ; Mean Value - 3.0075459480254807\n",
            "df_train_norm shape (12060910, 17)\n",
            " na_x - len 354, shape (24, 13) \n",
            " a_x - len 11\n",
            " na_x - len 351, shape (24, 13) \n",
            " a_x - len 13\n",
            " na_x - len 361, shape (24, 13) \n",
            " a_x - len 5\n",
            " na_x - len 353, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 357, shape (24, 13) \n",
            " a_x - len 8\n",
            " na_x - len 361, shape (24, 13) \n",
            " a_x - len 4\n",
            " na_x - len 339, shape (24, 13) \n",
            " a_x - len 26\n",
            " na_x - len 342, shape (24, 13) \n",
            " a_x - len 23\n",
            " na_x - len 311, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 365, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 363, shape (24, 13) \n",
            " a_x - len 2\n",
            " na_x - len 340, shape (24, 13) \n",
            " a_x - len 25\n",
            " na_x - len 268, shape (24, 13) \n",
            " a_x - len 9\n",
            " na_x - len 225, shape (24, 13) \n",
            " a_x - len 141\n",
            " na_x - len 252, shape (24, 13) \n",
            " a_x - len 71\n",
            " na_x - len 341, shape (24, 13) \n",
            " a_x - len 24\n",
            " na_x - len 335, shape (24, 13) \n",
            " a_x - len 30\n",
            " na_x - len 224, shape (24, 13) \n",
            " a_x - len 142\n",
            " na_x - len 343, shape (24, 13) \n",
            " a_x - len 22\n",
            " na_x - len 339, shape (24, 13) \n",
            " a_x - len 11\n",
            " na_x - len 365, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 332, shape (24, 13) \n",
            " a_x - len 34\n",
            " na_x - len 339, shape (24, 13) \n",
            " a_x - len 26\n",
            " na_x - len 365, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 224, shape (24, 13) \n",
            " a_x - len 142\n",
            " na_x - len 178, shape (24, 13) \n",
            " a_x - len 187\n",
            " na_x - len 225, shape (24, 13) \n",
            " a_x - len 141\n",
            " na_x - len 222, shape (24, 13) \n",
            " a_x - len 88\n",
            " na_x - len 364, shape (24, 13) \n",
            " a_x - len 2\n",
            " na_x - len 223, shape (24, 13) \n",
            " a_x - len 143\n",
            " na_x - len 364, shape (24, 13) \n",
            " a_x - len 1\n",
            " na_x - len 306, shape (24, 13) \n",
            " a_x - len 5\n",
            " na_x - len 364, shape (24, 13) \n",
            " a_x - len 1\n",
            " na_x - len 341, shape (24, 13) \n",
            " a_x - len 24\n",
            " na_x - len 224, shape (24, 13) \n",
            " a_x - len 142\n",
            " na_x - len 351, shape (24, 13) \n",
            " a_x - len 14\n",
            " na_x - len 365, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 343, shape (24, 13) \n",
            " a_x - len 22\n",
            " na_x - len 352, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 338, shape (24, 13) \n",
            " a_x - len 28\n",
            " na_x - len 363, shape (24, 13) \n",
            " a_x - len 1\n",
            " na_x - len 361, shape (24, 13) \n",
            " a_x - len 5\n",
            " na_x - len 364, shape (24, 13) \n",
            " a_x - len 1\n",
            " na_x - len 311, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 363, shape (24, 13) \n",
            " a_x - len 1\n",
            " na_x - len 140, shape (24, 13) \n",
            " a_x - len 225\n",
            " na_x - len 298, shape (24, 13) \n",
            " a_x - len 1\n",
            " na_x - len 225, shape (24, 13) \n",
            " a_x - len 141\n",
            " na_x - len 307, shape (24, 13) \n",
            " a_x - len 33\n",
            " na_x - len 364, shape (24, 13) \n",
            " a_x - len 2\n",
            " na_x - len 311, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 341, shape (24, 13) \n",
            " a_x - len 24\n",
            " na_x - len 364, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 300, shape (24, 13) \n",
            " a_x - len 66\n",
            " na_x - len 236, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 307, shape (24, 13) \n",
            " a_x - len 58\n",
            " na_x - len 245, shape (24, 13) \n",
            " a_x - len 121\n",
            " na_x - len 363, shape (24, 13) \n",
            " a_x - len 2\n",
            " na_x - len 365, shape (24, 13) \n",
            " a_x - len 1\n",
            " na_x - len 311, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 365, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 127, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 365, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 296, shape (24, 13) \n",
            " a_x - len 70\n",
            " na_x - len 363, shape (24, 13) \n",
            " a_x - len 2\n",
            " na_x - len 364, shape (24, 13) \n",
            " a_x - len 2\n",
            " na_x - len 310, shape (24, 13) \n",
            " a_x - len 13\n",
            " na_x - len 294, shape (24, 13) \n",
            " a_x - len 30\n",
            " na_x - len 225, shape (24, 13) \n",
            " a_x - len 141\n",
            " na_x - len 343, shape (24, 13) \n",
            " a_x - len 22\n",
            " na_x - len 364, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 299, shape (24, 13) \n",
            " a_x - len 27\n",
            " na_x - len 328, shape (24, 13) \n",
            " a_x - len 19\n",
            " na_x - len 311, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 364, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 365, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 364, shape (24, 13) \n",
            " a_x - len 2\n",
            " na_x - len 311, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 363, shape (24, 13) \n",
            " a_x - len 2\n",
            " na_x - len 330, shape (24, 13) \n",
            " a_x - len 35\n",
            " na_x - len 303, shape (24, 13) \n",
            " a_x - len 49\n",
            " na_x - len 311, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 365, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 333, shape (24, 13) \n",
            " a_x - len 32\n",
            " na_x - len 364, shape (24, 13) \n",
            " a_x - len 1\n",
            " na_x - len 352, shape (24, 13) \n",
            " a_x - len 13\n",
            " na_x - len 351, shape (24, 13) \n",
            " a_x - len 9\n",
            " na_x - len 349, shape (24, 13) \n",
            " a_x - len 12\n",
            " na_x - len 225, shape (24, 13) \n",
            " a_x - len 141\n",
            " na_x - len 342, shape (24, 13) \n",
            " a_x - len 23\n",
            " na_x - len 224, shape (24, 13) \n",
            " a_x - len 142\n",
            " na_x - len 365, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 335, shape (24, 13) \n",
            " a_x - len 31\n",
            " na_x - len 328, shape (24, 13) \n",
            " a_x - len 38\n",
            " na_x - len 354, shape (24, 13) \n",
            " a_x - len 11\n",
            " na_x - len 104, shape (24, 13) \n",
            " a_x - len 247\n",
            " na_x - len 311, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 364, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 307, shape (24, 13) \n",
            " a_x - len 57\n",
            " na_x - len 365, shape (24, 13) \n",
            " a_x - len 1\n",
            " na_x - len 365, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 224, shape (24, 13) \n",
            " a_x - len 142\n",
            " na_x - len 364, shape (24, 13) \n",
            " a_x - len 1\n",
            " na_x - len 364, shape (24, 13) \n",
            " a_x - len 2\n",
            " na_x - len 361, shape (24, 13) \n",
            " a_x - len 4\n",
            " na_x - len 355, shape (24, 13) \n",
            " a_x - len 10\n",
            " na_x - len 289, shape (24, 13) \n",
            " a_x - len 26\n",
            " na_x - len 365, shape (24, 13) \n",
            " a_x - len 1\n",
            " na_x - len 360, shape (24, 13) \n",
            " a_x - len 6\n",
            " na_x - len 356, shape (24, 13) \n",
            " a_x - len 9\n",
            " na_x - len 305, shape (24, 13) \n",
            " a_x - len 61\n",
            " na_x - len 342, shape (24, 13) \n",
            " a_x - len 23\n",
            " na_x - len 363, shape (24, 13) \n",
            " a_x - len 1\n",
            " na_x - len 311, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 336, shape (24, 13) \n",
            " a_x - len 30\n",
            " na_x - len 221, shape (24, 13) \n",
            " a_x - len 145\n",
            " na_x - len 364, shape (24, 13) \n",
            " a_x - len 1\n",
            " na_x - len 364, shape (24, 13) \n",
            " a_x - len 2\n",
            " na_x - len 354, shape (24, 13) \n",
            " a_x - len 11\n",
            " na_x - len 364, shape (24, 13) \n",
            " a_x - len 1\n",
            " na_x - len 364, shape (24, 13) \n",
            " a_x - len 2\n",
            " na_x - len 96, shape (24, 13) \n",
            " a_x - len 0\n",
            " na_x - len 344, shape (24, 13) \n",
            " a_x - len 21\n",
            " na_x - len 284, shape (24, 13) \n",
            " a_x - len 82\n",
            " na_x - len 346, shape (24, 13) \n",
            " a_x - len 13\n",
            " na_x - len 351, shape (24, 13) \n",
            " a_x - len 14\n",
            " na_x - len 362, shape (24, 13) \n",
            " a_x - len 4\n",
            " na_x - len 339, shape (24, 13) \n",
            " a_x - len 26\n",
            " na_x - len 363, shape (24, 13) \n",
            " a_x - len 2\n",
            " na_x - len 362, shape (24, 13) \n",
            " a_x - len 4\n",
            " na_x - len 97, shape (24, 13) \n",
            " a_x - len 235\n",
            " na_x - len 254, shape (24, 13) \n",
            " a_x - len 4\n",
            " na_x - len 350, shape (24, 13) \n",
            " a_x - len 16\n",
            " na_x - len 360, shape (24, 13) \n",
            " a_x - len 6\n",
            " na_x - len 224, shape (24, 13) \n",
            " a_x - len 142\n",
            " na_x - len 355, shape (24, 13) \n",
            " a_x - len 10\n",
            " na_x - len 348, shape (24, 13) \n",
            " a_x - len 17\n",
            " na_x - len 298, shape (24, 13) \n",
            " a_x - len 67\n",
            " na_x - len 301, shape (24, 13) \n",
            " a_x - len 65\n",
            " na_x - len 337, shape (24, 13) \n",
            " a_x - len 29\n",
            " na_x - len 338, shape (24, 13) \n",
            " a_x - len 27\n",
            " na_x - len 336, shape (24, 13) \n",
            " a_x - len 30\n",
            " na_x - len 225, shape (24, 13) \n",
            " a_x - len 141\n",
            " na_x - len 359, shape (24, 13) \n",
            " a_x - len 5\n",
            " na_x - len 311, shape (24, 13) \n",
            " a_x - len 0\n",
            "ouput_x shape 4859 , (24, 13)\n",
            "ouput_x shape 46182 , (24, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jf42zRP_DPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data(train_percent, val_percent, test_percent, output_non_anom_x, output_non_anom_y ):\n",
        "\n",
        "  # train dataset\n",
        "  X_train_non_anom = output_non_anom_x[:int((len(output_non_anom_y)*train_percent))]\n",
        "  Y_train_non_anom = output_non_anom_y[:int((len(output_non_anom_y)*train_percent))]\n",
        "\n",
        "  # validation dataset\n",
        "  X_val_non_anom = output_non_anom_x[int((len(output_non_anom_y)*train_percent)):-int((len(output_non_anom_y)*test_percent) )]\n",
        "  Y_val_non_anom = output_non_anom_y[int((len(output_non_anom_y)*train_percent)):-int((len(output_non_anom_y)*test_percent) )]\n",
        "\n",
        "  # test dataset                                                          \n",
        "  X_test_non_anom = output_non_anom_x[-int(len(output_non_anom_y)*test_percent):]\n",
        "  Y_test_non_anom = output_non_anom_y[-int(len(output_non_anom_y)*test_percent):]     \n",
        "\n",
        "  return  X_train_non_anom, Y_train_non_anom, X_val_non_anom, Y_val_non_anom, X_test_non_anom, Y_test_non_anom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz3ufudCuoTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_percent = 0.8\n",
        "val_percent = 0.1\n",
        "test_percent = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GruIOx1A8fZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_non_anom, Y_train_non_anom, X_val_non_anom, Y_val_non_anom, X_test_non_anom, Y_test_non_anom = split_data(train_percent, val_percent, test_percent, non_anom_x, non_anom_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWK1WtPNlHBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import percentile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuyAqPWmiFMl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "e959a105-d253-4a06-a554-44e1204b7ce2"
      },
      "source": [
        "!pip install pyod"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyod in /usr/local/lib/python3.6/dist-packages (0.8.1)\n",
            "Requirement already satisfied: combo in /usr/local/lib/python3.6/dist-packages (from pyod) (0.1.1)\n",
            "Requirement already satisfied: suod in /usr/local/lib/python3.6/dist-packages (from pyod) (0.0.4)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.6/dist-packages (from pyod) (1.18.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from pyod) (0.16.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from pyod) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from pyod) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pyod) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pyod) (3.2.2)\n",
            "Requirement already satisfied: numba>=0.35 in /usr/local/lib/python3.6/dist-packages (from pyod) (0.48.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyod) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyod) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyod) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyod) (1.2.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.35->pyod) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.35->pyod) (49.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAkhCyTSGdJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_percent_anom = 0.1\n",
        "val_percent_anom = 0.2\n",
        "test_percent_anom = 0.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyEGpQ--mipl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_anom, Y_train_anom, X_val_anom, Y_val_anom, X_test_anom, Y_test_anom = split_data(train_percent_anom, val_percent_anom, test_percent_anom, anom_x, anom_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHo88wkDcsbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_data_pyod(Y_train_non_anom, Y_train_anom, Y_val_non_anom, Y_val_anom, Y_test_non_anom,Y_test_anom):\n",
        "\n",
        "  # Anomalous  - train data\n",
        "  arr_1 = np.zeros((1,24))\n",
        "  for i in range(len(Y_train_anom)):\n",
        "    arr_1 = np.concatenate([arr_1, Y_train_anom[i].reshape(1, 24)], axis=0)\n",
        "  arr_1 = np.delete(arr_1, (0), axis=0)\n",
        "\n",
        "  # Non- Anomalous  - train data\n",
        "  arr_0 = np.zeros((1,24))\n",
        "  for i in range(len(Y_train_non_anom)):\n",
        "    arr_0 = np.concatenate([arr_0, Y_train_non_anom[i].reshape(1, 24)], axis=0)\n",
        "  arr_0 = np.delete(arr_0, (0), axis=0)\n",
        "\n",
        "  X_train = np.concatenate([arr_0, arr_1], axis=0)\n",
        "\n",
        "  Y_train = np.zeros((arr_0.shape[0])) \n",
        "  Y_train = np.concatenate([Y_train, np.ones((arr_1.shape[0]))], axis=0)\n",
        "\n",
        "  # Anomalous  - val data\n",
        "  val_arr_1 = np.zeros((1,24))\n",
        "  for i in range(len(Y_val_anom)):\n",
        "    val_arr_1 = np.concatenate([val_arr_1, Y_val_anom[i].reshape(1, 24)], axis=0)\n",
        "  val_arr_1 = np.delete(val_arr_1, (0), axis=0)\n",
        "\n",
        "  # Non- Anomalous  - val data\n",
        "  val_arr_0 = np.zeros((1,24))\n",
        "  for i in range(len(Y_val_non_anom)):\n",
        "    val_arr_0 = np.concatenate([val_arr_0, Y_val_non_anom[i].reshape(1, 24)], axis=0)\n",
        "  val_arr_0 = np.delete(val_arr_0, (0), axis=0)\n",
        "\n",
        "  X_val = np.concatenate([val_arr_0, val_arr_1], axis=0)\n",
        "\n",
        "  Y_val = np.zeros((val_arr_0.shape[0])) \n",
        "  Y_val = np.concatenate([Y_val, np.ones((val_arr_1.shape[0]))], axis=0)\n",
        "\n",
        "\n",
        "  # Anomalous  - test data\n",
        "  test_arr_1 = np.zeros((1,24))\n",
        "  for i in range(len(Y_test_anom)):\n",
        "    test_arr_1 = np.concatenate([test_arr_1, Y_test_anom[i].reshape(1, 24)], axis=0)\n",
        "  test_arr_1 = np.delete(test_arr_1, (0), axis=0)\n",
        "\n",
        "  # Non- Anomalous  - val data\n",
        "  test_arr_0 = np.zeros((1,24))\n",
        "  for i in range(len(Y_test_non_anom)):\n",
        "    test_arr_0 = np.concatenate([test_arr_0, Y_test_non_anom[i].reshape(1, 24)], axis=0)\n",
        "  test_arr_0 = np.delete(test_arr_0, (0), axis=0)\n",
        "\n",
        "  X_test = np.concatenate([test_arr_0, test_arr_1], axis=0)\n",
        "\n",
        "  Y_test = np.zeros((test_arr_0.shape[0])) \n",
        "  Y_test = np.concatenate([Y_test, np.ones((test_arr_1.shape[0]))], axis=0)\n",
        "\n",
        "  return X_train, Y_train, X_val, Y_val, X_test, Y_test, arr_0, arr_1, val_arr_0, val_arr_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ1JjUlwhh4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, Y_train, X_val, Y_val, X_test, Y_test, arr_0, arr_1, val_arr_0, val_arr_1 = create_data_pyod(Y_train_non_anom, Y_train_anom, Y_val_non_anom, Y_val_anom, Y_test_non_anom,Y_test_anom)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3nxsBg1s83G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "from sklearn.utils import shuffle\n",
        "x_train, y_train = shuffle(X_train, Y_train)\n",
        "x_val, y_val = shuffle(X_val, Y_val)\n",
        "x_test, y_test = shuffle(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG9T24oEJ_mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Compare all detection algorithms \n",
        "\"\"\"\n",
        "# Author: Yue Zhao <zhaoy@cmu.edu>\n",
        "# License: BSD 2 clause\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# supress warnings for clean output\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from numpy import percentile\n",
        "\n",
        "# Import all models\n",
        "from pyod.utils import data\n",
        "from pyod.models.abod import ABOD\n",
        "from pyod.models.cblof import CBLOF\n",
        "from pyod.models.feature_bagging import FeatureBagging\n",
        "from pyod.models.hbos import HBOS\n",
        "from pyod.models.iforest import IForest\n",
        "from pyod.models.knn import KNN\n",
        "from pyod.models.lof import LOF\n",
        "from pyod.models.loci import LOCI\n",
        "from pyod.models.mcd import MCD\n",
        "from pyod.models.ocsvm import OCSVM\n",
        "from pyod.models.pca import PCA\n",
        "from pyod.models.sos import SOS\n",
        "from pyod.models.lscp import LSCP\n",
        "from pyod.models.cof import COF\n",
        "from pyod.models.sod import SOD\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS7LkWyPPeKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Define the number of inliers and outliers  validation data \n",
        "training_outliers_fraction = arr_1.shape[0] / arr_0.shape[0]\n",
        "# training_outliers_fraction = 0\n",
        "outliers_fraction = val_arr_1.shape[0] / val_arr_0.shape[0]\n",
        "\n",
        "\n",
        "clusters_separation = [0]\n",
        "\n",
        "# initialize a set of detectors for LSCP\n",
        "detector_list = [LOF(n_neighbors=5), LOF(n_neighbors=10), LOF(n_neighbors=15),\n",
        "                 LOF(n_neighbors=20), LOF(n_neighbors=25), LOF(n_neighbors=30),\n",
        "                 LOF(n_neighbors=35), LOF(n_neighbors=40), LOF(n_neighbors=45),\n",
        "                 LOF(n_neighbors=50)]\n",
        "\n",
        "# # Show the statics of the data\n",
        "# print('Number of inliers: %i' % n_inliers)\n",
        "# print('Number of outliers: %i' % n_outliers)\n",
        "# print(\n",
        "#     'Ground truth shape is {shape}. Outlier are 1 and inliers are 0.\\n'.format(\n",
        "#         shape=ground_truth.shape))\n",
        "# print(ground_truth, '\\n')\n",
        "\n",
        "random_state = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uucVWEg2PW0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define nine outlier detection tools to be compared\n",
        "classifiers = {\n",
        "    # 'Angle-based Outlier Detector (ABOD)':\n",
        "    #     ABOD(contamination=outliers_fraction),\n",
        "    'Cluster-based Local Outlier Factor (CBLOF)':\n",
        "        CBLOF(contamination=training_outliers_fraction,\n",
        "              check_estimator=False, random_state=random_state),\n",
        "    'Feature Bagging':\n",
        "        FeatureBagging(LOF(n_neighbors=35),\n",
        "                       contamination=training_outliers_fraction,\n",
        "                       random_state=random_state),\n",
        "    'Histogram-base Outlier Detection (HBOS)': HBOS(\n",
        "        contamination=training_outliers_fraction),\n",
        "    'Isolation Forest': IForest(contamination=training_outliers_fraction,\n",
        "                                random_state=random_state),\n",
        "    'K Nearest Neighbors (KNN)': KNN(\n",
        "        contamination=training_outliers_fraction),\n",
        "    'Average KNN': KNN(method='mean',\n",
        "                       contamination=training_outliers_fraction),\n",
        "    'Median KNN': KNN(method='median',\n",
        "                      contamination=training_outliers_fraction),\n",
        "    'Local Outlier Factor (LOF)':\n",
        "        LOF(n_neighbors=35, contamination=training_outliers_fraction),\n",
        "    # 'Local Correlation Integral (LOCI)':\n",
        "    #     LOCI(contamination=outliers_fraction),\n",
        "    'Minimum Covariance Determinant (MCD)': MCD(\n",
        "        contamination=training_outliers_fraction, random_state=random_state),\n",
        "    'One-class SVM (OCSVM)': OCSVM(contamination=training_outliers_fraction),\n",
        "    'Principal Component Analysis (PCA)': PCA(\n",
        "        contamination=training_outliers_fraction, random_state=random_state),\n",
        "    # 'Stochastic Outlier Selection (SOS)': SOS(\n",
        "    #     contamination=training_outliers_fraction),\n",
        "    # # 'Locally Selective Combination (LSCP)': LSCP(\n",
        "    # #     detector_list, contamination=outliers_fraction,\n",
        "    # #     random_state=random_state),\n",
        "    # 'Connectivity-Based Outlier Factor (COF)':\n",
        "    #     COF(n_neighbors=35, contamination=training_outliers_fraction),\n",
        "    # 'Subspace Outlier Detection (SOD)':\n",
        "    #     SOD(contamination=training_outliers_fraction),\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RriI8mQdPYRG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4e30baea-725a-4f7e-a6ab-b9a4c11c009d"
      },
      "source": [
        "# Show all detectors\n",
        "for i, clf in enumerate(classifiers.keys()):\n",
        "    print('Model', i + 1, clf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model 1 Cluster-based Local Outlier Factor (CBLOF)\n",
            "Model 2 Feature Bagging\n",
            "Model 3 Histogram-base Outlier Detection (HBOS)\n",
            "Model 4 Isolation Forest\n",
            "Model 5 K Nearest Neighbors (KNN)\n",
            "Model 6 Average KNN\n",
            "Model 7 Median KNN\n",
            "Model 8 Local Outlier Factor (LOF)\n",
            "Model 9 Minimum Covariance Determinant (MCD)\n",
            "Model 10 One-class SVM (OCSVM)\n",
            "Model 11 Principal Component Analysis (PCA)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEjPflykwbLR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2398967f-d479-4fce-9bb2-bc3f701b50c1"
      },
      "source": [
        "# Fit the models with the generated data and\n",
        "# compare model performances\n",
        "\n",
        "# Fit the model\n",
        "for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
        "    print()\n",
        "    print(i + 1, 'fitting', clf_name)\n",
        "    # fit the train data and tag outliers\n",
        "    clf.fit(x_train)\n",
        "\n",
        "    # get the prediction labels and outlier scores of the training data\n",
        "    y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
        "    y_train_scores = clf.decision_scores_  # raw outlier scores\n",
        "\n",
        "    # evaluate and print the results\n",
        "    print(\"\\nOn Training Data:\")\n",
        "    data.evaluate_print(clf_name, y_train, y_train_scores)\n",
        "\n",
        "    ## VALIDATION DATA \n",
        "    # get the prediction labels and outlier scores of the validation data\n",
        "    y_val_scores = clf.decision_function(x_val)  # outlier scores\n",
        "\n",
        "    # set threshold on the validation data \n",
        "    threshold = percentile(y_val_scores, 100 * outliers_fraction)\n",
        "    print(\"threshold : \", threshold)\n",
        "    # convet scores to label using the calculated labels \n",
        "    y_val_pred = (y_val_scores > threshold).astype('int')\n",
        "\n",
        "    # evaluate and print the results on Validation data \n",
        "    print(\"\\nOn Validation Data:\")\n",
        "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_val, y_val_pred).ravel()\n",
        "    p = tp/(tp+fp)\n",
        "    r = tp/(tp+fn)\n",
        "    f1 = 2*p*r/(p+r)\n",
        "    print(\"Precision : {}\".format(p))\n",
        "    print(\"Recall : {}\".format(r))\n",
        "    print(\"F1 score : {}\".format(f1))\n",
        "\n",
        "    ## TEST DATA \n",
        "    # get the prediction labels and outlier scores of the validation data\n",
        "    y_test_scores = clf.decision_function(x_test)  # outlier scores\n",
        "\n",
        "    # convet scores to label using the calculated labels  (threshold from validation data )\n",
        "    y_test_pred = (y_test_scores > threshold).astype('int')\n",
        "\n",
        "    # evaluate and print the results on Validation data \n",
        "    print(\"\\nOn Test Data:\")\n",
        "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_test, y_test_pred).ravel()\n",
        "    p = tp/(tp+fp)\n",
        "    r = tp/(tp+fn)\n",
        "    f1 = 2*p*r/(p+r)\n",
        "    print(\"Precision : {}\".format(p))\n",
        "    print(\"Recall : {}\".format(r))\n",
        "    print(\"F1 score : {}\".format(f1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "1 fitting Cluster-based Local Outlier Factor (CBLOF)\n",
            "\n",
            "On Training Data:\n",
            "Cluster-based Local Outlier Factor (CBLOF) ROC:0.9237, precision @ rank n:0.699\n",
            "threshold :  0.8672493261705692\n",
            "\n",
            "On Validation Data:\n",
            "Precision : 0.21386497507929317\n",
            "Recall : 0.9701952723535457\n",
            "F1 score : 0.3504733617969185\n",
            "\n",
            "On Test Data:\n",
            "Precision : 0.4860492727812407\n",
            "Recall : 0.9629520729197295\n",
            "F1 score : 0.6460203175855607\n",
            "\n",
            "2 fitting Feature Bagging\n",
            "\n",
            "On Training Data:\n",
            "Feature Bagging ROC:0.4178, precision @ rank n:0.1464\n",
            "threshold :  1.0\n",
            "\n",
            "On Validation Data:\n",
            "Precision : 0.08008405323371469\n",
            "Recall : 0.35251798561151076\n",
            "F1 score : 0.13051750380517504\n",
            "\n",
            "On Test Data:\n",
            "Precision : 0.23612181958365458\n",
            "Recall : 0.3601881799470744\n",
            "F1 score : 0.2852485737571312\n",
            "\n",
            "3 fitting Histogram-base Outlier Detection (HBOS)\n",
            "\n",
            "On Training Data:\n",
            "Histogram-base Outlier Detection (HBOS) ROC:0.871, precision @ rank n:0.0149\n",
            "threshold :  32.94614937480233\n",
            "\n",
            "On Validation Data:\n",
            "Precision : 0.2084277299501586\n",
            "Recall : 0.9455292908530318\n",
            "F1 score : 0.3415630220902172\n",
            "\n",
            "On Test Data:\n",
            "Precision : 0.48157579388855604\n",
            "Recall : 0.9453102028815055\n",
            "F1 score : 0.6380867321623499\n",
            "\n",
            "4 fitting Isolation Forest\n",
            "\n",
            "On Training Data:\n",
            "Isolation Forest ROC:0.9021, precision @ rank n:0.1923\n",
            "threshold :  -0.28581954013818894\n",
            "\n",
            "On Validation Data:\n",
            "Precision : 0.21091980063434526\n",
            "Recall : 0.9568345323741008\n",
            "F1 score : 0.345646927789122\n",
            "\n",
            "On Test Data:\n",
            "Precision : 0.48735119047619047\n",
            "Recall : 0.9629520729197295\n",
            "F1 score : 0.6471692520501926\n",
            "\n",
            "5 fitting K Nearest Neighbors (KNN)\n",
            "\n",
            "On Training Data:\n",
            "K Nearest Neighbors (KNN) ROC:0.2829, precision @ rank n:0.1526\n",
            "threshold :  0.10363840531595099\n",
            "\n",
            "On Validation Data:\n",
            "Precision : 0.06909832351608518\n",
            "Recall : 0.31346351490236385\n",
            "F1 score : 0.11323556710599592\n",
            "\n",
            "On Test Data:\n",
            "Precision : 0.2127780946948089\n",
            "Recall : 0.32902087621287857\n",
            "F1 score : 0.2584295612009238\n",
            "\n",
            "6 fitting Average KNN\n",
            "\n",
            "On Training Data:\n",
            "Average KNN ROC:0.2808, precision @ rank n:0.1526\n",
            "threshold :  0.09029263526061239\n",
            "\n",
            "On Validation Data:\n",
            "Precision : 0.06864521975532396\n",
            "Recall : 0.3114080164439877\n",
            "F1 score : 0.11249303879710414\n",
            "\n",
            "On Test Data:\n",
            "Precision : 0.20846905537459284\n",
            "Recall : 0.3199059100264628\n",
            "F1 score : 0.25243619489559166\n",
            "\n",
            "7 fitting Median KNN\n",
            "\n",
            "On Training Data:\n",
            "Median KNN ROC:0.2819, precision @ rank n:0.1505\n",
            "threshold :  0.09196784456344147\n",
            "\n",
            "On Validation Data:\n",
            "Precision : 0.06909832351608518\n",
            "Recall : 0.31346351490236385\n",
            "F1 score : 0.11323556710599592\n",
            "\n",
            "On Test Data:\n",
            "Precision : 0.2108479755538579\n",
            "Recall : 0.32461040870332253\n",
            "F1 score : 0.2556443209447725\n",
            "\n",
            "8 fitting Local Outlier Factor (LOF)\n",
            "\n",
            "On Training Data:\n",
            "Local Outlier Factor (LOF) ROC:0.4239, precision @ rank n:0.1588\n",
            "threshold :  1.0\n",
            "\n",
            "On Validation Data:\n",
            "Precision : 0.08098508169547715\n",
            "Recall : 0.35149023638232274\n",
            "F1 score : 0.13163972286374134\n",
            "\n",
            "On Test Data:\n",
            "Precision : 0.23936792820912992\n",
            "Recall : 0.36077624228168187\n",
            "F1 score : 0.28779172041749734\n",
            "\n",
            "9 fitting Minimum Covariance Determinant (MCD)\n",
            "\n",
            "On Training Data:\n",
            "Minimum Covariance Determinant (MCD) ROC:0.4099, precision @ rank n:0.1588\n",
            "threshold :  10.351933085752622\n",
            "\n",
            "On Validation Data:\n",
            "Precision : 0.07780548628428928\n",
            "Recall : 0.32065775950668035\n",
            "F1 score : 0.12522576760987356\n",
            "\n",
            "On Test Data:\n",
            "Precision : 0.23518556489645273\n",
            "Recall : 0.33725374889738313\n",
            "F1 score : 0.2771200773133607\n",
            "\n",
            "10 fitting One-class SVM (OCSVM)\n",
            "\n",
            "On Training Data:\n",
            "One-class SVM (OCSVM) ROC:0.9275, precision @ rank n:0.2069\n",
            "threshold :  -83.47520410620749\n",
            "\n",
            "On Validation Data:\n",
            "Precision : 0.2149977344811962\n",
            "Recall : 0.9753340184994861\n",
            "F1 score : 0.352329682569148\n",
            "\n",
            "On Test Data:\n",
            "Precision : 0.48766519823788546\n",
            "Recall : 0.9764775066157013\n",
            "F1 score : 0.6504749779649397\n",
            "\n",
            "11 fitting Principal Component Analysis (PCA)\n",
            "\n",
            "On Training Data:\n",
            "Principal Component Analysis (PCA) ROC:0.9085, precision @ rank n:0.6784\n",
            "threshold :  155943.70134661847\n",
            "\n",
            "On Validation Data:\n",
            "Precision : 0.21295876755777074\n",
            "Recall : 0.9660842754367934\n",
            "F1 score : 0.348988305179135\n",
            "\n",
            "On Test Data:\n",
            "Precision : 0.4915129151291513\n",
            "Recall : 0.9791237871214349\n",
            "F1 score : 0.6544811320754718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08bAZp_1kViq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}